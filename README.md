# Goodreads Book Crawler Project Output Directory

This folder contains all output files generated by the Goodreads Book Crawler application, including CSV, JSON, and other data files.

## How to Use the Application

### 1. Setup

- Ensure you have Python 3.8+ installed.
- Install dependencies by running:
  ```powershell
  pip install -r src/crawlers/requirements.txt
  ```

### 2. Running the Crawler

- From the project root, run the book crawler to extract book data:
  ```powershell
  python src/crawlers/book_crawler_genre.py
  ```
- This will generate `books.json` and `books.csv` in the `output/` folder.

### 3. Running the Streamlit Dashboard

- To visualize and explore the extracted data, run:
  ```powershell
  streamlit run src/crawlers/streamlit_app.py
  ```
- Open the provided local URL in your browser to access the dashboard.

### 4. Expanding the Genres

- To crawl additional genres, edit the `allowed_genres` list in `src/crawlers/book_crawler_genre.py`.
- Add or remove genre names as needed. Save the file and rerun the crawler script.

### 5. Output Files

- `books.json`: Contains structured book data in JSON format.
- `books.csv`: Contains the same data in CSV format for spreadsheet use.

## Project Authors & Roles

- **Member 1 – Crawlability Specialist**
- **Member 2 – Content Extractor**
- **Member 3 – JS & API Handler**
- **Member 4 – Visual & Report Designer**
- **Member 5 – Documentation & Deployment**

---

For any issues, please check the documentation or contact the relevant team member.
# Web_Crawling_-_Scrapping
